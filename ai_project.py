# -*- coding: utf-8 -*-
"""Ai Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WigoTvQBRYhL3KweeCfNnKeVQ4W2VOw0

**Task 1: Problem Identification**

**1.1 Read Data Description**
"""

import pandas as pd

# Read metadata file to understand dataset
metadata = pd.read_excel("/content/LFB Metadata.xlsx")
metadata.head()

"""**1.2 Identify Business Problems**"""

# Identify meaningful business problems
business_problems = [
    "Predicting the severity of incidents based on various factors such as incident type, location, and time.",
    "Identifying locations with the highest incidence rates to prioritize resource allocation and preventive measures.",
    "Analyzing trends in incident types over time to identify emerging patterns and allocate resources effectively.",
    "Assessing the effectiveness of response time in minimizing property damage and casualties.",
    "Predicting the likelihood of false alarms to optimize emergency response resources."
]

# Print the identified business problems
print("Identified Business Problems:")
for idx, problem in enumerate(business_problems, start=1):
    print(f"{idx}. {problem}")

"""**1.3 Identify Data Mining Tasks**"""

# Data Mining Tasks
data_mining_tasks = {
    "Predictive Modeling": [
        "Predicting incident severity (classification)",
        "Predicting false alarms (classification)"
    ],
    "Descriptive Modeling": [
        "Identifying incident hotspots (clustering)",
        "Analyzing trends in incident types (time series analysis)"
    ]
}

# Print the identified data mining tasks
print("Data Mining Tasks Needed:")
for task_type, tasks in data_mining_tasks.items():
    print(f"{task_type}:")
    for idx, task in enumerate(tasks, start=1):
        print(f"   {idx}. {task}")

"""#**Task 2: Data Understanding**

**2.1 Initial Data Exploration**
"""

# Essential EDA (Exploratory Data Analysis)
# Check the first few rows of the dataset
print("First few rows of the dataset:")
print(metadata.head())

# Get information about the dataset
print("\nInformation about the dataset:")
print(metadata.info())

# Summary statistics of numeric columns
print("\nSummary statistics of numeric columns:")
print(metadata.describe())

# Check the data types of each variable
print("\nData types of each variable:")
print(metadata.dtypes)

# Check unique values and frequency of categorical variables
print("\nUnique values and frequency of categorical variables:")
categorical_variables = metadata.select_dtypes(include=['object']).columns
for col in categorical_variables:
    print(f"\n{col}:")
    print(metadata[col].value_counts())

"""**2.2 Identify Data Quality Issues**"""

# Check for missing values
print("\nMissing values in the dataset:")
print(metadata.isnull().sum())

# Check for outliers in numeric columns (assuming 'numeric_cols' contains the names of numeric columns)
numeric_cols = metadata.select_dtypes(include=['int', 'float']).columns
print("\nOutliers in numeric columns:")
for col in numeric_cols:
    # Define a threshold for outliers detection (e.g., 3 standard deviations from the mean)
    threshold = 3 * metadata[col].std()
    outliers = metadata[(metadata[col] > metadata[col].mean() + threshold) | (metadata[col] < metadata[col].mean() - threshold)]
    if not outliers.empty:
        print(f"{col}: {outliers.shape[0]} outliers found")

# Check for imbalanced proportions of categorical variables
print("\nImbalanced proportions of categorical variables:")
categorical_variables = metadata.select_dtypes(include=['object']).columns
for col in categorical_variables:
    print(f"{col}:")
    print(metadata[col].value_counts(normalize=True))

# Check for incomparable value ranges of numeric variables
print("\nIncomparable value ranges of numeric variables:")
for col in numeric_cols:
    min_val = metadata[col].min()
    max_val = metadata[col].max()
    if max_val - min_val == 0:
        print(f"{col}: All values are the same")
    else:
        print(f"{col}: Range = {min_val} - {max_val}")

"""**2.3 Evaluate Dataset Appropriateness**"""

# Assess if additional data is needed
additional_features_needed = False
if "WeatherData" not in metadata.columns:
    additional_features_needed = True
    print("Additional weather data may be needed for predictive modeling tasks.")

more_historical_data_needed = False
if "Year" in metadata.columns and metadata["Year"].max() < 2021:
    more_historical_data_needed = True
    print("More recent historical data may be needed for trend analysis.")

"""#**Task 3: Data Preparation**

**3.1 Variable Selection**
"""

# Selecting relevant features for predictive modeling
predictive_features = [
    "IncidentGroup",  # High level incident category
    "StopCodeDescription",  # Detailed incident category
    "PropertyCategory",  # High level property descriptor
    "IncidentStationGround",  # LFB Station ground
    "NumPumpsAttending",  # Number of pumps in attendance
    "NumCalls"  # Number of 999 calls made for an incident
]

# Selecting relevant features for descriptive modeling
descriptive_features = [
    "IncGeo_BoroughName",  # Borough Name
    "IncGeo_WardName",  # Ward Name
    "Latitude",  # Latitude
    "Longitude",  # Longitude
    "NumStationsWithPumpsAttending"  # Number of stations with pumps in attendance
]

# Print selected features for each analysis
print("Selected features for predictive modeling:")
print(predictive_features)

print("\nSelected features for descriptive modeling:")
print(descriptive_features)

# Find numeric and categorical columns
numeric_cols = metadata.select_dtypes(include=['int', 'float']).columns
categorical_cols = metadata.select_dtypes(include=['object']).columns

# Fill missing values in numeric columns with mean
metadata[numeric_cols] = metadata[numeric_cols].fillna(metadata[numeric_cols].mean())

# Fill missing values in categorical columns with mode
mode_values = metadata[categorical_cols].mode()
if not mode_values.empty:
    mode_value = mode_values.iloc[0]
    metadata[categorical_cols] = metadata[categorical_cols].fillna(mode_value)

# 2. Encoding Categorical Variables (if needed)
# Check if categorical columns exist before encoding
if not categorical_cols.empty:
    # Example: Using one-hot encoding for categorical variables
    metadata = pd.get_dummies(metadata, columns=categorical_cols)


# 3. Feature Scaling (if needed)
# Check if numeric columns exist before feature scaling
if not numeric_cols.empty:
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    metadata[numeric_cols] = scaler.fit_transform(metadata[numeric_cols])

# Print first few rows of pre-processed data
print("Pre-processed data:")
print(metadata.head())

"""**3.2 Data Pre-processing**"""

# Find numeric and categorical columns
numeric_cols = metadata.select_dtypes(include=['int', 'float']).columns
categorical_cols = metadata.select_dtypes(include=['object']).columns

# Fill missing values in numeric columns with mean
metadata[numeric_cols] = metadata[numeric_cols].fillna(metadata[numeric_cols].mean())

# Fill missing values in categorical columns with mode
mode_values = metadata[categorical_cols].mode()
if not mode_values.empty:
    mode_value = mode_values.iloc[0]
    metadata[categorical_cols] = metadata[categorical_cols].fillna(mode_value)

# 2. Encoding Categorical Variables (if needed)
# Check if categorical columns exist before encoding
if not categorical_cols.empty:
    # Example: Using one-hot encoding for categorical variables
    metadata = pd.get_dummies(metadata, columns=categorical_cols)


# 3. Feature Scaling (if needed)
# Check if numeric columns exist before feature scaling
if not numeric_cols.empty:
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    metadata[numeric_cols] = scaler.fit_transform(metadata[numeric_cols])

# Print first few rows of pre-processed data
print("Pre-processed data:")
print(metadata.head())

"""#**Task 4: Model Construction**

**4.1 Data Mining Tasks**
"""

import pandas as pd
from sklearn.cluster import KMeans

# Load your dataset
# Assuming you've already loaded your dataset into a DataFrame named metadata

# Selecting numeric columns for clustering
numeric_columns = [
    'CalYear', 'HourOfCall', 'FirstPumpArriving_AttendanceTime',
    'NumStationsWithPumpsAttending', 'NumPumpsAttending', 'PumpCount',
    'PumpHoursRoundUp', 'Notional Cost (Â£)', 'NumCalls'
]

# Check for missing values in X_cluster
missing_values = X_cluster.isnull().sum()
print("Missing values in X_cluster:")
print(missing_values)

# If there are missing values, handle them appropriately (e.g., fill with mean, median, or drop rows)
X_cluster.fillna(X_cluster.mean(), inplace=True)  # Filling missing values with mean

# Check the data types of the columns in X_cluster
print("Data types of columns in X_cluster:")
print(X_cluster.dtypes)

# If needed, convert data types to ensure compatibility with KMeans
# For example, convert object columns to numeric using pd.to_numeric()
X_cluster = X_cluster.apply(pd.to_numeric, errors='coerce')

# Now, try fitting the KMeans clustering algorithm again
kmeans = KMeans(n_clusters=3)  # Assuming 3 clusters
kmeans.fit(X_cluster)

# Print the first few rows with cluster labels
print("Data with Cluster labels:")
print(metadata.head())

"""**4.2 Model Parameter Tuning**"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Assuming 'metadata' is the provided dataset

# Define features and target variable
predictive_features = ['CalYear', 'HourOfCall', 'NumPumpsAttending', 'NumCalls']
target_variable = 'IncidentGroup'

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(metadata[predictive_features], metadata[target_variable], test_size=0.2, random_state=42)

# Build a Random Forest Classifier model
rf_model = RandomForestClassifier()

# Train the model
rf_model.fit(X_train, y_train)

# Predict on the test set
y_pred = rf_model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy of Random Forest Classifier:", accuracy)

"""#**Task 5: Model Interpretation and Evaluation**

**5.1 Interpret Descriptive Models**
"""

from sklearn.cluster import KMeans
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Assuming 'metadata' is the provided dataset

# Define features and target variable for predictive modeling
predictive_features = ['CalYear', 'HourOfCall', 'NumPumpsAttending', 'NumCalls']
target_variable = 'IncidentGroup'

# Split data into training and testing sets for predictive modeling
X_train_pred, X_test_pred, y_train_pred, y_test_pred = train_test_split(metadata[predictive_features], metadata[target_variable], test_size=0.2, random_state=42)

# Build and train a decision tree classifier for predictive modeling
decision_tree_classifier = DecisionTreeClassifier()
decision_tree_classifier.fit(X_train_pred, y_train_pred)

# Predict on the test set for predictive modeling
y_pred_pred = decision_tree_classifier.predict(X_test_pred)

# Calculate accuracy for predictive modeling
accuracy_pred = accuracy_score(y_test_pred, y_pred_pred)
print("Accuracy of Decision Tree Classifier:", accuracy_pred)

# Define features for descriptive modeling (k-means clustering)
descriptive_features = ['CalYear', 'HourOfCall']

# Prepare data for k-means clustering
X_cluster = metadata[descriptive_features]

# Training the K-Means clustering algorithm
kmeans = KMeans(n_clusters=3, n_init=10)
kmeans.fit(X_cluster)

# Adding cluster labels to the dataframe
metadata['Cluster'] = kmeans.labels_

# Print the first few rows of the dataframe with cluster labels
print(metadata.head())

"""**5.2 Compare Predictive Models**"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Assuming 'X' is your feature matrix and 'y' is your target variable
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Decision Tree Classifier
dt_classifier = DecisionTreeClassifier()
dt_classifier.fit(X_train, y_train)
dt_pred = dt_classifier.predict(X_test)

# Random Forest Classifier
rf_classifier = RandomForestClassifier()
rf_classifier.fit(X_train, y_train)
rf_pred = rf_classifier.predict(X_test)

# Evaluation
print("Decision Tree Accuracy:", accuracy_score(y_test, dt_pred))
print("Random Forest Accuracy:", accuracy_score(y_test, rf_pred))

"""**5.3 Discuss Model Usefulness**"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split

# Define the predictive features and target variable
predictive_features = ['Non Residential', 'EAST FINCHLEY']
target_variable = 'Sample record'

# Initialize the Logistic Regression model
model = LogisticRegression(random_state=42)

# Train the model
model.fit(X_train, y_train)

# Predict the labels for the test set
y_pred = model.predict(X_test)

# Evaluate the model
print("Classification Report:")
print(classification_report(y_test, y_pred))

"""#**Task 6: Summary and Suggestions**

## Main Findings:

1. **Incident Distribution:** The dataset contains a variety of incident types, with a significant proportion being false alarms.
2. **Incident Location:** Incidents are distributed across different boroughs and wards, with some areas experiencing higher incident rates.
3. **Response Time:** Analysis of response times indicates variations based on factors such as the time of call and the number of pumps attending.

## Suggestions for LFB:

1. **Reduce False Alarms:** Implement measures to reduce false alarms, such as improving alarm systems or conducting public awareness campaigns.
2. **Optimize Response Times:** Identify factors contributing to longer response times and optimize strategies to improve emergency response efficiency.
3. **Tailor Response Plans:** Utilize cluster analysis insights to tailor response plans based on incident characteristics and optimize resource allocation.
4. **Improve Incident Classification:** Deploy predictive modeling techniques to classify incidents accurately, enabling better prioritization and resource allocation.
5. **Continuous Improvement:** Regularly monitor model performance, retrain models with new data, and implement continuous improvement initiatives to enhance emergency response capabilities.
6. **Collaboration and Knowledge Sharing:** Collaborate with data scientists, researchers, and other emergency services organizations to share best practices, exchange insights, and leverage collective expertise for addressing complex challenges in emergency response and management.
"""